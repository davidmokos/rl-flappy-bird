{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fa513-0be3-43fd-868e-de36c7fa1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flappy-bird-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a66a8-4cd4-4110-8fa3-09d9575c088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4dd436c5-8ab3-4ccd-a030-0d8c7470ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.11/site-packages (from opencv-python) (1.25.2)\n",
      "Downloading opencv_python-4.8.1.78-cp37-abi3-macosx_11_0_arm64.whl (33.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a44b197d-fd27-4555-92fa-a60da1756101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_env\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "import time\n",
    "# from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81042da9-ff8c-4fe4-987e-3aa875eb56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "53313ad5-fc55-41c3-afdf-de91376e44aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 72)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, reward, terminated, _, _ = env.step(action)\n",
    "\n",
    "env.observation_space.shape\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(obs, interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "# obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "232e33e9-3fae-4e65-8247-84260e5082cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs):\n",
    "    epsilon_threshold = random.uniform(0, 1)\n",
    "    if epsilon_threshold > 0.92:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7799875-7c14-4465-ae3d-f2d3d2740c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_rewards = []\n",
    "\n",
    "for i in range(10):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    epoch_reward = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        action = get_action(obs)\n",
    "        obs, reward, terminated, _, _ = env.step(action)\n",
    "        epoch_reward += reward\n",
    "\n",
    "    epoch_rewards.append(epoch_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc1267-7cde-4124-886a-8fba2fcdd890",
   "metadata": {},
   "source": [
    "## Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bfcb8ff8-2473-4a08-9ff2-107ee7054575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gymnasium.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        print(env.unwrapped.get_action_meanings())\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.env.reset(seed=seed, options=options)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(seed=seed, options=options)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(seed=seed, options=options)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gymnasium.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset(seed=seed, options=options)\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gymnasium.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples image to 100x72\n",
    "    Greyscales image\n",
    "\n",
    "    Returns numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gymnasium.spaces.Box(low=0, high=255, shape=(100, 72, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 800 * 576 * 3:\n",
    "            img = np.reshape(frame, [800, 576, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (72, 100), interpolation=cv2.INTER_AREA)\n",
    "        # x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(resized_screen, [100, 72, 1])\n",
    "        x_t = x_t.astype(np.uint8)\n",
    "        return x_t\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gymnasium.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gymnasium.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gymnasium.ObservationWrapper):\n",
    "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gymnasium.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gymnasium.spaces.Box(\n",
    "            old_space.low.repeat(n_steps, axis=0),\n",
    "            old_space.high.repeat(n_steps, axis=0), dtype=dtype\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset(seed=seed, options=options))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gymnasium.make(env_name, render_mode=\"human\")\n",
    "    # env = MaxAndSkipEnv(env)\n",
    "    # env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    # env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "564ec268-a006-49cf-9241-e4b2f66ccbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5ba8bb72-7c13-4b41-a3e7-5d713c2306f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"FlappyBird-v0\")\n",
    "\n",
    "\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10 ** 4 * 4 # Maximum number of experiences stored in replay memory\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE_FREQ = 1000 # How many frames in between syncing target DQN with behaviour DQN\n",
    "LEARNING_STARTS = 50000 # Number of experiences to add to replay memory before training network\n",
    "\n",
    "EPSILON_DECAY = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9434d9c2-daaf-4826-86f8-d3435faa66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eeed6e69-d764-4bf2-a6aa-c287ca3d716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, replay_memory):\n",
    "        self.env = env\n",
    "        self.replay_memory = replay_memory\n",
    "        self._reset()\n",
    "        self.last_action = 0\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Select action\n",
    "        Execute action and step environment\n",
    "        Add state/action/reward to experience replay\n",
    "        \"\"\"\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        new_state = new_state\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.replay_memory.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2d63e9da-b6d5-4723-93e3-f4ade06c76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(batch, net, target_net, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Calculate MSE between actual state action values,\n",
    "    and expected state action values from DQN\n",
    "    \"\"\"\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.long().unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = target_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5f537b06-737e-4b6d-b41f-213b34d7016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayMemory will require 9.22gb of GPU RAM\n",
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2880, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m frame_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(EPSILON_FINAL, EPSILON_START \u001b[38;5;241m-\u001b[39m frame_idx \u001b[38;5;241m/\u001b[39m EPSILON_DECAY)\n\u001b[0;32m---> 26\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     total_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[0;32mIn[146], line 29\u001b[0m, in \u001b[0;36mAgent.play_step\u001b[0;34m(self, net, epsilon, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_v\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# do step in the environment\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m new_state, reward, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     31\u001b[0m new_state \u001b[38;5;241m=\u001b[39m new_state\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "print(\"ReplayMemory will require {}gb of GPU RAM\".format(round(REPLAY_SIZE * 32 * 100 * 72 / 1e+9, 2)))\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "env = make_env(\"FlappyBird-v0\")\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "print(net)\n",
    "\n",
    "replay_memory = ExperienceReplay(REPLAY_SIZE)\n",
    "agent = Agent(env, replay_memory)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "best_mean_reward = None\n",
    "frame_idx = 0\n",
    "timestep_frame = 0\n",
    "timestep = time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - timestep_frame) / (time.time() - timestep)\n",
    "        timestep_frame = frame_idx\n",
    "        timestep = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"{} frames: done {} games, mean reward {}, eps {}, speed {} f/s\".format(\n",
    "            frame_idx, len(total_rewards), round(mean_reward, 3), round(epsilon,2), round(speed, 2)))\n",
    "\n",
    "    if len(replay_memory) < LEARNING_STARTS:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = replay_memory.sample(BATCH_SIZE)\n",
    "    loss_t = calculate_loss(batch, net, target_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
